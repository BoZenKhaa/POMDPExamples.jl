{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Heuristic Policy\n",
    "In this section, we will walk through how to define a POMDP policy of your own. For more details on POMDPs and their policies, please consult Chapter 6 of the DMU textbook [1] We will define a simple heuristic policy that takes the action that maximises the expected single-step reward, given the current belief state. We will also compare it against a policy that chooses actions at random. Please look at the documentation of [POMDPPolicies.jl](https://github.com/JuliaPOMDP/POMDPPolicies.jl) for more on the code structure of a policy object that is compatible with POMDPs.jl. We will use the explicit TigerPOMDP model - see [this](http://localhost:8888/notebooks/POMDPExamples/notebooks/Defining-a-POMDP-with-the-Explicit-Interface.ipynb) notebook for more on that.\n",
    "\n",
    "[1] Kochenderfer, Mykel J. Decision Making Under Uncertainty: Theory and Application. MIT Press, 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/shushman/.julia/compiled/v1.0/POMDPs/GAotg.ji for POMDPs [a93abf59-7444-517b-a68a-c42f96afdd7d]\n",
      "└ @ Base loading.jl:1184\n",
      "┌ Info: Recompiling stale cache file /home/shushman/.julia/compiled/v1.0/POMDPPolicies/2WG4l.ji for POMDPPolicies [182e52fb-cfd0-5e46-8c26-fd0667c990f4]\n",
      "└ @ Base loading.jl:1184\n",
      "┌ Info: Recompiling stale cache file /home/shushman/.julia/compiled/v1.0/POMDPModels/GHWgR.ji for POMDPModels [355abbd5-f08e-5560-ac9e-8b5f2592a0ca]\n",
      "└ @ Base loading.jl:1184\n",
      "WARNING: Method definition GridWorld(Any...) in module POMDPModels at deprecated.jl:29 overwritten at deprecated.jl:53.\n"
     ]
    }
   ],
   "source": [
    "using POMDPs\n",
    "using POMDPPolicies # For defining a policy\n",
    "using POMDPModels # For the TigerPOMDP Model\n",
    "using BeliefUpdaters # To use DiscreteUpdater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a HeuristicPolicy type that only requires the POMDP instance and the set of valid actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct HeuristicPolicy{P<:POMDP, A} <: Policy\n",
    "    pomdp::P\n",
    "    action_map::Vector{A}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeuristicPolicy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function HeuristicPolicy(pomdp::POMDP)\n",
    "    HeuristicPolicy(pomdp, actions(pomdp))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overriding the POMDPs.jl action function\n",
    "Now we will define the `action` function, which specifies the behavior of the heuristic policy. It requires the belief state to be represented as a `DiscreteBelief`, i.e. a Probability Mass Function over individual states. It computes the expected single-step reward for each action, given the current belief state, and chooses the maximum one. Note that we must use `POMDPs.action` to override the `action` method of `POMDPs.jl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.action(p::HeuristicPolicy, b::DiscreteBelief)\n",
    "    max_value = -Inf\n",
    "    best_idx = 1\n",
    "    for i = 1:n_actions(p.pomdp)\n",
    "        a = p.action_map[i]\n",
    "        action_val = 0.0\n",
    "        for (bel,state) in zip(b.b, b.state_list)\n",
    "            action_val += bel*reward(p.pomdp, state, a)\n",
    "        end\n",
    "        \n",
    "        if action_val > max_value\n",
    "            best_idx = i\n",
    "            max_value = action_val\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return p.action_map[best_idx]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking the Heuristic Policy\n",
    "Note that unlike the other examples on using *solvers*, here we have already specified a *policy*. Therefore, we can just evaluate the policy on a problem, in this case, `TigerPOMDP`(defined in [POMDPModels](https://github.com/JuliaPOMDP/POMDPModels.jl)). We define the POMDP problem and create the policies based on it.\n",
    "\n",
    "Since we only care about the discounted reward, we can use the rollout simulator defined in [POMDPSimulators](https://github.com/JuliaPOMDP/POMDPSimulators.jl). Checkout this [notebook](https://github.com/JuliaPOMDP/POMDPExamples.jl/blob/master/notebooks/Running-Simulations.ipynb) for ways to use the other simulators as well. Finally, we can compare the expected discounted rewards and see how the heuristic policy does quite better than random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TigerPOMDP(-1.0, -100.0, 10.0, 0.85, 0.95)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pomdp = TigerPOMDP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeuristicPolicy{TigerPOMDP,Int64}(TigerPOMDP(-1.0, -100.0, 10.0, 0.85, 0.95), [0, 1, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heur_pol = HeuristicPolicy(pomdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a random policy as a benchmark\n",
    "rand_policy = RandomPolicy(pomdp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/shushman/.julia/compiled/v1.0/POMDPSimulators/i1HOp.ji for POMDPSimulators [e0d0a172-29c6-5d4e-96d0-f262df5d01fd]\n",
      "└ @ Base loading.jl:1184\n"
     ]
    }
   ],
   "source": [
    "using POMDPSimulators\n",
    "rollout_sim = RolloutSimulator(max_steps=10);\n",
    "history_heur = simulate(rollout_sim, pomdp, heur_pol, DiscreteUpdater(pomdp));\n",
    "history_rand = simulate(rollout_sim, pomdp, rand_policy, DiscreteUpdater(pomdp));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history_heur = 9.583949041798828\n",
      "history_rand = -207.38836272128898\n"
     ]
    }
   ],
   "source": [
    "@show history_heur;\n",
    "@show history_rand;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
